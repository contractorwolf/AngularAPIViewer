{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real-Time-Voice-Cloning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/contractorwolf/AngularAPIViewer/blob/master/Real_Time_Voice_Cloning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yk3PMfBuZhS",
        "colab_type": "text"
      },
      "source": [
        "Make sure GPU is enabled\n",
        "Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU\n",
        "\n",
        "I have made edits to the orginal codebase, located here:\n",
        "https://github.com/CorentinJ/Real-Time-Voice-Cloning\n",
        "\n",
        "\n",
        "My edits were mainly related to getting the code running fully on colab.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdeo9XZq_eZJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhunyJSod_UT",
        "colab_type": "code",
        "outputId": "faebee1c-86a4-4e87-b104-18b970a11cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/contractorwolf/Real-Time-Voice-Cloning.git  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Real-Time-Voice-Cloning'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 2364 (delta 16), reused 19 (delta 8), pack-reused 2337\u001b[K\n",
            "Receiving objects: 100% (2364/2364), 360.25 MiB | 13.11 MiB/s, done.\n",
            "Resolving deltas: 100% (1300/1300), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE6btDZWeFV0",
        "colab_type": "code",
        "outputId": "7f1d0431-f24b-4029-ce09-b548c55d6686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd Real-Time-Voice-Cloning/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Real-Time-Voice-Cloning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO3TS3P9LP82",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "install requirements for this repository\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AVd9vLKeKm6",
        "colab_type": "code",
        "outputId": "10769346-af1b-43c9-decb-dc37b9fee7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu<=1.14.0,>=1.10.0 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 51kB/s \n",
            "\u001b[?25hRequirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.3.10)\n",
            "Collecting visdom (from -r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/c4/5f5356fd57ae3c269e0e31601ea6487e0622fedc6756a591e4a5fd66cc7a/visdom-0.1.8.8.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 41.6MB/s \n",
            "\u001b[?25hCollecting webrtcvad (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/34/e2de2d97f3288512b9ea56f92e7452f8207eb5a0096500badf9dfd48f5e6/webrtcvad-2.0.10.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 30.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: librosa>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.6.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.16.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (4.28.1)\n",
            "Collecting sounddevice (from -r requirements.txt (line 10))\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/f2/7cb487ac7171dfade8af7a368bd8806ecff82016f4ac9894835cd7de9ecc/sounddevice-0.3.13-py2.py3-none-any.whl\n",
            "Collecting Unidecode (from -r requirements.txt (line 11))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (2.1.0)\n",
            "Collecting PyQt5 (from -r requirements.txt (line 13))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/bd/8a0c863802449f35ad9ca21a1b73190639c206758b4b5e2425617fc99ce9/PyQt5-5.13.0-5.13.0-cp35.cp36.cp37.cp38-abi3-manylinux1_x86_64.whl (62.1MB)\n",
            "\u001b[K     |████████████████████████████████| 62.1MB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (0.70.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (0.40.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from umap-learn->-r requirements.txt (line 2)) (0.21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (4.5.3)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (17.0.0)\n",
            "Collecting torchfile (from visdom->-r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client (from visdom->-r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 63.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (4.3.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (0.2.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (4.4.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (2.1.8)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.6/dist-packages (from sounddevice->-r requirements.txt (line 10)) (1.12.3)\n",
            "Collecting PyQt5_sip<13,>=4.19.14 (from PyQt5->-r requirements.txt (line 13))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/15/055d5667166b1d8c04ad159216be7fa254897fafd79e749bc263cb86f802/PyQt5_sip-4.19.18-cp36-cp36m-manylinux1_x86_64.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from multiprocess->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->-r requirements.txt (line 15)) (0.29.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.15.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (41.2.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->visdom->-r requirements.txt (line 3)) (0.46)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from CFFI>=1.0->sounddevice->-r requirements.txt (line 10)) (2.19)\n",
            "Building wheels for collected packages: visdom, webrtcvad, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.8-cp36-none-any.whl size=1350603 sha256=ea5408b1d50623c808ada255af8f5bb11f3194a269a4b5f4a9eb779afdf7c480\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/87/ce/a5023722374ca73b57fc8d4284ba6f973c01219b3c385a07e0\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp36-cp36m-linux_x86_64.whl size=71270 sha256=2390e2e2e44a12ae51d418dddc4b7259db6e5bc58cfbaecf06092ccf6adaa9e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/2a/18/bd1aec41cac7c3051fe95d92a6ed446122ea31dc713c432fa1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=7592b3ffbbdd48307c20968aad06acc6da056d402dff33599fc19c78cadfe5b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built visdom webrtcvad torchfile\n",
            "Installing collected packages: tensorflow-gpu, torchfile, websocket-client, visdom, webrtcvad, sounddevice, Unidecode, PyQt5-sip, PyQt5\n",
            "Successfully installed PyQt5-5.13.0 PyQt5-sip-4.19.18 Unidecode-1.1.1 sounddevice-0.3.13 tensorflow-gpu-1.14.0 torchfile-0.1.0 visdom-0.1.8.8 webrtcvad-2.0.10 websocket-client-0.56.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOW3ZDZFFczG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Download the pretrained voice model that used audiobooks (text and voice) as the source. This source lacks tone, but usable. You are going to fine tune this model to your voice recording to make the cloned voice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuwgOQlPeN8a",
        "colab_type": "code",
        "outputId": "db9a62fe-585e-4fc4-a874-9ff61fb05a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#!gdown https://drive.google.com/uc?id=1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc < original model hosted here\n",
        "!gdown https://drive.google.com/uc?id=1dmjCLeh4w5xWOc7daA2GQPF1IqXsYTxi\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dmjCLeh4w5xWOc7daA2GQPF1IqXsYTxi\n",
            "To: /content/Real-Time-Voice-Cloning/pretrained.zip\n",
            "384MB [00:14, 26.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMLA1NKF7XC",
        "colab_type": "text"
      },
      "source": [
        "unpack the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKLpYfRkfyjX",
        "colab_type": "code",
        "outputId": "9d8b4989-1742-48e8-afca-26328e550604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!unzip pretrained.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  pretrained.zip\n",
            "   creating: encoder/saved_models/\n",
            "  inflating: encoder/saved_models/pretrained.pt  \n",
            "   creating: synthesizer/saved_models/\n",
            "   creating: synthesizer/saved_models/logs-pretrained/\n",
            "   creating: synthesizer/saved_models/logs-pretrained/taco_pretrained/\n",
            " extracting: synthesizer/saved_models/logs-pretrained/taco_pretrained/checkpoint  \n",
            "  inflating: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000.data-00000-of-00001  \n",
            "  inflating: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000.index  \n",
            "  inflating: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000.meta  \n",
            "   creating: vocoder/saved_models/\n",
            "   creating: vocoder/saved_models/pretrained/\n",
            "  inflating: vocoder/saved_models/pretrained/pretrained.pt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8keiJSdGOhu",
        "colab_type": "text"
      },
      "source": [
        "install required audio libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84mPqfQGgM6t",
        "colab_type": "code",
        "outputId": "ed686260-a180-4a7a-b2b5-08cbfa18bd31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!apt-get install libportaudio2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 68%\r\rReading package lists... 68%\r\rReading package lists... 68%\r\rReading package lists... 68%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 64.6 kB of archives.\n",
            "After this operation, 215 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Fetched 64.6 kB in 1s (44.3 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebCD8x10G3GE",
        "colab_type": "text"
      },
      "source": [
        "First imports libraries for recording the sample audio file (voice to be cloned).  After the files and code are loaded it immediately starts recording a file named \"sample.wav\" to be used in the next steps by the encoder. Read the text below until the block of code has completed (10 seconds).\n",
        "\n",
        "READ THIS:\n",
        "\n",
        "# The Pro is an upgraded version of the Classic. In the pro model you get a electrical system, super powerful rear hub brushless motor with tons of torque, an easy-access battery with port for phone charings, and Fat Tires with puncture-resistant casings. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p9pvtZKe4d2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d5b23c7-87bc-42a5-95ef-dfaaa6e7cc79"
      },
      "source": [
        "# all imports\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "\n",
        "print(\"RECORD 10 SECONDS OF TALKING, keep talking until you see 'sample.wav' output after this message\")\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(filename, sec=3):\n",
        "  display(Javascript(RECORD))\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  with open(filename,'wb') as f:\n",
        "    f.write(b)\n",
        "  return 'FILE RECORDED: ' + filename\n",
        "\n",
        "record('sample.wav', sec=10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RECORD 10 SECONDS OF TALKING, keep talking until you see 'sample.wav' output after this message\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FILE RECORDED: sample.wav'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH-wDHzLHZ2q",
        "colab_type": "text"
      },
      "source": [
        "Executing the python file below will ask for a local file (or assume sample.wav) and create a voice signature from that sample. If you you need to upload an example for demo_colab.py you will need to upload it to the root folder and use a path like /voicesample.m4a. In the file it will ask you for the text to \"say\" using the voice signature.  It will then create a WAV file with a random filename (demo_output_##.wav).\n",
        "\n",
        "NEEDED:\n",
        "input for file execution should include: voice_signature_file, incoming_text, output_filename\n",
        "\n",
        "The output will be located here: \n",
        "/content/Real-Time-Voice-Cloning/demo_output_##.wav\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOiGYfpAf2qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1230a7e-1726-42c9-ad88-fb0d60db8dbe"
      },
      "source": [
        "#!python demo_cli.py --no_sound\n",
        "#!python demo_colab.py --no_sound\n",
        "#!python demo_colab_sample.py --no_sound\n",
        "\n",
        "!python demo_colab_sample.py --no_sound  \\\n",
        "  --textin=\"this is going to be my text for the demo it should sound like me\" \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0901 04:21:35.464736 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:91: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "Arguments:\n",
            "    enc_model_fpath:   encoder/saved_models/pretrained.pt\n",
            "    syn_model_dir:     synthesizer/saved_models/logs-pretrained\n",
            "    voc_model_fpath:   vocoder/saved_models/pretrained/pretrained.pt\n",
            "    low_mem:           False\n",
            "    no_sound:          True\n",
            "    out:               output.wav\n",
            "    textin:            this is going to be my text for the demo it should sound like me\n",
            "    voicein:           /content/Real-Time-Voice-Cloning/sample.wav\n",
            "\n",
            "\n",
            "args.textin:  this is going to be my text for the demo it should sound like me\n",
            "\n",
            "args.voicein:  /content/Real-Time-Voice-Cloning/sample.wav\n",
            "\n",
            "args.out:  output.wav\n",
            "Running a test of your configuration...\n",
            "\n",
            "Found 1 GPUs available. Using GPU 0 (Tesla T4) of compute capability 7.5 with 15.8Gb total memory.\n",
            "\n",
            "Preparing the encoder, the synthesizer and the vocoder...\n",
            "Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
            "Found synthesizer \"pretrained\" trained to step 278000\n",
            "Building Wave-RNN\n",
            "Trainable Parameters: 4.481M\n",
            "Loading model weights at vocoder/saved_models/pretrained/pretrained.pt\n",
            "Testing your configuration with small inputs.\n",
            "\tTesting the encoder...\n",
            "\tTesting the synthesizer... (loading the model will output a lot of text)\n",
            "W0901 04:21:43.831043 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/inference.py:57: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "Constructing model: Tacotron\n",
            "W0901 04:21:43.831326 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/tacotron2.py:15: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0901 04:21:43.835568 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/tacotron2.py:21: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0901 04:21:43.844532 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/tacotron.py:86: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0901 04:21:43.851002 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/tacotron.py:123: The name tf.train.replica_device_setter is deprecated. Please use tf.compat.v1.train.replica_device_setter instead.\n",
            "\n",
            "W0901 04:21:43.851470 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/tacotron.py:135: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0901 04:21:43.852013 140611097855872 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0901 04:21:43.862810 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:112: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0901 04:21:43.863910 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:421: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "W0901 04:21:44.086711 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:422: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W0901 04:21:44.145401 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:425: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0901 04:21:44.254516 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:236: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "W0901 04:21:44.254836 140611097855872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0901 04:21:44.336904 140611097855872 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0901 04:21:44.796118 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:156: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
            "\n",
            "W0901 04:21:44.796404 140611097855872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0901 04:21:45.189051 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/attention.py:158: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
            "\n",
            "W0901 04:21:45.189521 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/attention.py:161: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "W0901 04:21:45.190098 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:305: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0901 04:21:45.311032 140611097855872 deprecation.py:323] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:269: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0901 04:21:45.671398 140611097855872 ag_logging.py:145] Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7fe1c0e60cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7fe1c0e60cc0>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7fe1c0e60cc0>>, which Python reported as:\n",
            "    def __call__(self, inputs, state, scope=None):\n",
            "        \"\"\"Runs vanilla LSTM Cell and applies zoneout.\n",
            "        \"\"\"\n",
            "        # Apply vanilla LSTM\n",
            "        output, new_state = self._cell(inputs, state, scope)\n",
            "\n",
            "        if self.state_is_tuple:\n",
            "            (prev_c, prev_h) = state\n",
            "            (new_c, new_h) = new_state\n",
            "        else:\n",
            "            num_proj = self._cell._num_units if self._cell._num_proj is None else \\\n",
            "\t\t\t\tself._cell._num_proj\n",
            "            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n",
            "            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n",
            "            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n",
            "            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n",
            "\n",
            "        # Apply zoneout\n",
            "        if self.is_training:\n",
            "            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (\n",
            "\t\t\t# probability to mask activations)!\n",
            "            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c,\n",
            "                                                         (1 - self._zoneout_cell)) + prev_c\n",
            "            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h,\n",
            "                                                            (1 - self._zoneout_outputs)) + prev_h\n",
            "\n",
            "        else:\n",
            "            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n",
            "            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n",
            "\n",
            "        new_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,\n",
            "                                                                                                  h])\n",
            "\n",
            "        return output, new_state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "W0901 04:21:45.710030 140611097855872 ag_logging.py:145] Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7fe1c0cfb5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7fe1c0cfb5f8>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7fe1c0cfb5f8>>, which Python reported as:\n",
            "    def __call__(self, inputs, state, scope=None):\n",
            "        \"\"\"Runs vanilla LSTM Cell and applies zoneout.\n",
            "        \"\"\"\n",
            "        # Apply vanilla LSTM\n",
            "        output, new_state = self._cell(inputs, state, scope)\n",
            "\n",
            "        if self.state_is_tuple:\n",
            "            (prev_c, prev_h) = state\n",
            "            (new_c, new_h) = new_state\n",
            "        else:\n",
            "            num_proj = self._cell._num_units if self._cell._num_proj is None else \\\n",
            "\t\t\t\tself._cell._num_proj\n",
            "            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n",
            "            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n",
            "            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n",
            "            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n",
            "\n",
            "        # Apply zoneout\n",
            "        if self.is_training:\n",
            "            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (\n",
            "\t\t\t# probability to mask activations)!\n",
            "            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c,\n",
            "                                                         (1 - self._zoneout_cell)) + prev_c\n",
            "            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h,\n",
            "                                                            (1 - self._zoneout_outputs)) + prev_h\n",
            "\n",
            "        else:\n",
            "            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n",
            "            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n",
            "\n",
            "        new_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,\n",
            "                                                                                                  h])\n",
            "\n",
            "        return output, new_state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "initialisation done /gpu:0\n",
            "W0901 04:21:46.150110 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/tacotron.py:286: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "Initialized Tacotron model. Dimensions (? = dynamic shape): \n",
            "  Train mode:               False\n",
            "  Eval mode:                False\n",
            "  GTA mode:                 False\n",
            "  Synthesis mode:           True\n",
            "  Input:                    (?, ?)\n",
            "  device:                   0\n",
            "  embedding:                (?, ?, 512)\n",
            "  enc conv out:             (?, ?, 512)\n",
            "  encoder out (cond):       (?, ?, 768)\n",
            "  decoder out:              (?, ?, 80)\n",
            "  residual out:             (?, ?, 512)\n",
            "  projected residual out:   (?, ?, 80)\n",
            "  mel out:                  (?, ?, 80)\n",
            "  <stop_token> out:         (?, ?)\n",
            "  Tacotron Parameters       28.439 Million.\n",
            "Loading checkpoint: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000\n",
            "2019-09-01 04:21:46.151512: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2019-09-01 04:21:46.155251: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-01 04:21:46.155802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-01 04:21:46.156570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9ae0bb80 executing computations on platform CUDA. Devices:\n",
            "2019-09-01 04:21:46.156598: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-09-01 04:21:46.158025: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-09-01 04:21:46.158231: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9ae0b640 executing computations on platform Host. Devices:\n",
            "2019-09-01 04:21:46.158255: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-01 04:21:46.158416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-01 04:21:46.158933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-01 04:21:46.158996: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-01 04:21:46.341309: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-01 04:21:46.416992: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-01 04:21:46.432606: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-01 04:21:46.653049: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-01 04:21:46.783580: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-01 04:21:47.114533: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-01 04:21:47.114752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-01 04:21:47.115367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-01 04:21:47.115856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-01 04:21:47.115957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-01 04:21:47.116019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-01 04:21:47.116037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-01 04:21:47.116051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-01 04:21:47.116179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-01 04:21:47.116730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-01 04:21:47.117239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13495 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-09-01 04:21:47.243464: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-09-01 04:21:47.255771: W tensorflow/core/framework/allocator.cc:107] Allocation of 33554432 exceeds 10% of system memory.\n",
            "2019-09-01 04:21:47.327823: W tensorflow/core/framework/allocator.cc:107] Allocation of 33554432 exceeds 10% of system memory.\n",
            "W0901 04:21:47.440160 140611097855872 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/tacotron2.py:62: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0901 04:21:47.502357 140611097855872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-09-01 04:21:47.599928: W tensorflow/core/framework/allocator.cc:107] Allocation of 33554432 exceeds 10% of system memory.\n",
            "2019-09-01 04:21:47.625951: W tensorflow/core/framework/allocator.cc:107] Allocation of 33554432 exceeds 10% of system memory.\n",
            "2019-09-01 04:21:48.520196: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-01 04:21:49.313152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "\tTesting the vocoder...\n",
            "All test passed! You can now synthesize speech.\n",
            "\n",
            "\n",
            "This is a GUI-less example of interface to SV2TTS. The purpose of this script is to show how you can interface this project easily with your own. See the source code for an explanation of what is happening.\n",
            "\n",
            "Loaded file succesfully\n",
            "Created the embedding\n",
            "Caught exception: TypeError(\"object of type 'PosixPath' has no len()\",)\n",
            "Restarting\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-YjpfriJEYu",
        "colab_type": "text"
      },
      "source": [
        "Play output file which should be the incoming_text modeled with the voice signature file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyLdbUfks2lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio('/content/Real-Time-Voice-Cloning/output.wav') # load a local WAV file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZDUmM8VfRha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio('sample.wav') # load a local WAV file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF5z-iEgEcAG",
        "colab_type": "text"
      },
      "source": [
        "unused, check later\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnV2xXI-jEbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install youtube-dl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9VYtcYliOGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir UserAudio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5y58GgdiQdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# i think this is to use a youtube video as an audio source, but it would need to have a single voice speaker\n",
        "#!youtube-dl --extract-audio --audio-format wav -o \"UserAudio/test.wav\" link to youtube video here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}